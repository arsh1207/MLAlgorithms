{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.svm   \n",
    "import sklearn.preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import sklearn.metrics  \n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree.export import export_text\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.neural_network \n",
    "np.set_printoptions(precision=3, suppress=True) \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import average_precision_score, make_scorer, precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "class Retinopathy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def read_data(self):\n",
    "        data_set =  np.loadtxt('messidor_features.csv', delimiter=',', skiprows=0)\n",
    "        X = data_set[:,0:-1]\n",
    "        y = data_set[:,-1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "        \n",
    "    def preprocessing(self, X_train, X_test):\n",
    "        # preprocessing using standard scaler\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        return X_train, X_test\n",
    "        \n",
    "    def cv_SVM(self, X_train, y_train):\n",
    "        C_grid = [0.1, 1, 10]\n",
    "        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "        svm = sklearn.svm.SVC(kernel='rbf')\n",
    "        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(svm, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "\n",
    "    def cv_Knn(self, X_train, y_train):\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {'n_neighbors': range(1, 20)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(neigh, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "    \n",
    "    def cv_DT(self, X_train, y_train):\n",
    "        decision_tree = tree.DecisionTreeClassifier(random_state=0)\n",
    "        param_grid = {'max_depth': range(1, 20)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(decision_tree, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "        \n",
    "    def cv_RandomForest(self, X_train, y_train):\n",
    "        random_decision = RandomForestClassifier(random_state=0)\n",
    "        param_grid = {'n_jobs': range(1, 10)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(random_decision, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "        \n",
    "    def cv_adaBoost(self, X_train, y_train):\n",
    "        ada_boost = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "        param_grid = {'n_estimators': range(1, 100)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(ada_boost, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "    \n",
    "    def cv_logReg(self, X_train, y_train):\n",
    "        logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial',  max_iter=400)\n",
    "        param_grid = {'C': np.logspace(-4, 3, 20)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(logreg, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "        \n",
    "    def cv_GNB(self, X_train, y_train):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid ={}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(gnb, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "    \n",
    "    def cv_NN(self, X_train, y_train):\n",
    "        nn = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                                           solver='sgd', batch_size=100, max_iter=10,\n",
    "                                           learning_rate_init=.01, momentum=0.9, alpha=0.05,\n",
    "                                           verbose=True, random_state=0)\n",
    "\n",
    "        param_grid ={'hidden_layer_sizes' : (10,30,50,70,100)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(nn, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "    \n",
    "    def predict(self, model, X_test, y_test):\n",
    "        predict = model.predict(X_test)\n",
    "        accuracy = sklearn.metrics.accuracy_score(y_test, predict)\n",
    "        print(\"Final Accuracy on test data : \", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from io import StringIO\n",
    "import scipy\n",
    "import scipy.stats               # For reciprocal distribution\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import svm\n",
    "import sklearn.tree        # For DecisionTreeClassifier class\n",
    "import sklearn.ensemble    # For RandomForestClassifier class\n",
    "import sklearn.linear_model # For Logistic Classifier\n",
    "#from sklearn.neighbors import LSHForest\n",
    "import sklearn.naive_bayes #For Naive Bayes\n",
    "import sklearn.neural_network #For MLP classifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # Ignore sklearn deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)       # Ignore sklearn deprecation warnings\n",
    "np.set_printoptions(precision=20, suppress=True)\n",
    "\n",
    "class credit_card_defaults:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def scale_data(self,X):\n",
    "        scaler = preprocessing.StandardScaler().fit(X)\n",
    "        X = scaler.transform(X)\n",
    "        return(X,scaler)\n",
    "    \n",
    "    def AUPR_score_func(self,y_true, y_pred):\n",
    "        precision, recall, threshold = sklearn.metrics.precision_recall_curve(y_true, y_pred)\n",
    "        return sklearn.metrics.auc(recall, precision)\n",
    "    \n",
    "    def random_CV(self,clf,X,y,param_grid,n_iter,cv):\n",
    "        scorer_AUROC = make_scorer(sklearn.metrics.roc_auc_score) #using ROC for scoring criteria\n",
    "        scorer_PRECISON = make_scorer(self.AUPR_score_func) #Check PR curve Area due to imbalanced data set 1:4\n",
    "        scorer_ACCURACY = make_scorer(sklearn.metrics.accuracy_score) #Chec accuracy\n",
    "        \n",
    "        scoring = {'AUROC': scorer_AUROC, 'Accuracy':scorer_ACCURACY, 'AUPR':scorer_PRECISON }\n",
    "        \n",
    "        \n",
    "        print(\"Starting search\")\n",
    "        print(\"Score mechanism implemented by RandomizedCV - AUROC score + Accuracy + AUPR\")\n",
    "        random_search = model_selection.RandomizedSearchCV(clf, param_distributions = param_grid,n_iter = n_iter, cv = cv,\n",
    "                                           iid = False,verbose=1,scoring = scoring,refit = 'AUROC', n_jobs = -1)\n",
    "        random_search.fit(X, y)\n",
    "        print(\"best parameters:\", random_search.best_params_)\n",
    "        print(\"%.1f%% accuracy -for AUROC on validation sets (average)\" % (random_search.best_score_*100))\n",
    "        return random_search.best_params_\n",
    "    \n",
    "    def KNN(self,X,y):\n",
    "        print(\"Starting KNN classification training- Expected to take about 3 mins as its a hude data set\")\n",
    "       \n",
    "       # X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n",
    "        knn_clf = KNeighborsClassifier()\n",
    "        param_dist = {'n_neighbors': range(1,100),\n",
    "                     \"algorithm\" : ['ball_tree', 'kd_tree'],\n",
    "                    \"weights\" : ['uniform', 'distance'],\n",
    "                    \"leaf_size\" : range(1,100)}\n",
    "        print(\"Calling random_cv\")\n",
    "        return self.random_CV(knn_clf,X,y,param_dist,4,3)\n",
    "    \n",
    "    def SVM_clf(self,X,y):\n",
    "        print(\"Starting SVM classification\")\n",
    "        #svm_clf = svm.SVC()\n",
    "        svm_clf = svm.SVC()\n",
    "        param_dist = {\n",
    "            'C'     : scipy.stats.reciprocal(1.0, 1000.),\n",
    "            'kernel': ['linear','rbf','poly'],\n",
    "            'degree' : [2],\n",
    "            'gamma' : scipy.stats.reciprocal(0.01, 10.),\n",
    "            #'penalty': ['l1','l2'],\n",
    "            #'dual': [True,False],\n",
    "            'max_iter' : np.arange(5000,15000,200)\n",
    "            }\n",
    "        return self.random_CV(svm_clf,X,y,param_dist,10,3)\n",
    "    \n",
    "    \n",
    "    def DT_clf(self,X,y):\n",
    "        tree_clf = sklearn.tree.DecisionTreeClassifier()\n",
    "        param_dist = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"max_depth\" :[None,500,750,1000,1500,2000],\n",
    "            \"max_features\": [\"sqrt\",\"log2\",None]\n",
    "        }\n",
    "        return self.random_CV(tree_clf,X,y,param_dist,15,5)\n",
    "    \n",
    "    def RF_clf(self,X,y):\n",
    "        print(\"Random Forest Classifier Called\")\n",
    "        rf_clf = sklearn.ensemble.RandomForestClassifier()\n",
    "        param_dist = {\n",
    "            \"n_estimators\" : [10,25,50,75,100,125,150,175,200],\n",
    "            \"max_depth\" :[None,500,750,1000,1500,2000],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"max_features\": [\"sqrt\",\"log2\",None],\n",
    "            \"n_jobs\": [4],\n",
    "            \"warm_start\" : [True]\n",
    "           # \"bootstrap\": [True,False]\n",
    "        }\n",
    "        return self.random_CV(rf_clf,X,y,param_dist,10,3)\n",
    "    \n",
    "    def ADB_clf(self,X,y):\n",
    "        print(\"AdaBoost Classifier Called\")\n",
    "        adb_clf = sklearn.ensemble.AdaBoostClassifier()\n",
    "        param_dist = {\n",
    "            \"n_estimators\" : [50,100,150,200,300,500,750,1000],\n",
    "            \"algorithm\" : ['SAMME', 'SAMME.R'],\n",
    "           # \"random_state\" : [0]\n",
    "        }\n",
    "        return self.random_CV(adb_clf,X,y,param_dist,5,5)\n",
    "    \n",
    "    def LR_clf(self,X,y):\n",
    "        print(\"Logistic Reg Classifier Called\")\n",
    "        lr_clf = sklearn.linear_model.LogisticRegression()\n",
    "        param_dist = {\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            'C'     : scipy.stats.reciprocal(1.0, 1000.),\n",
    "            \"solver\" : ['lbfgs','sag','saga','newton-cg'],\n",
    "            \"penalty\" : ['l2'],\n",
    "            'max_iter' : np.arange(500,10000,500)\n",
    "        }\n",
    "        return self.random_CV(lr_clf,X,y,param_dist,10,3)\n",
    "    \n",
    "    \n",
    "    def NB_clf(self,X,y):\n",
    "        print(\"Naive Bayes Classifier called\")\n",
    "        nb_clf = sklearn.naive_bayes.GaussianNB()\n",
    "        param_dist = {\n",
    "            \"priors\": [None,[0.5,0.5],[0.6,0.4],[0.4,0.6],[0.3,0.7],[0.7,0.3]]\n",
    "        }\n",
    "        return self.random_CV(nb_clf,X,y,param_dist,5,5)\n",
    "    \n",
    "    def MLP_clf(self,X,y):\n",
    "        print(\"Neural Network / MLP Classifier called\")\n",
    "        mlp_clf = sklearn.neural_network.MLPClassifier()\n",
    "        param_dist = {\n",
    "            \"hidden_layer_sizes\" : [(100,), (200,),(100,50),(200,50),(200,100),(100,100,50)],\n",
    "            \"solver\" : ['sgd'],\n",
    "            \"learning_rate\" : ['constant','invscaling'],\n",
    "            \"max_iter\" : [200,300,500],\n",
    "            \"warm_start\" : [True],\n",
    "            \"activation\" :['tanh', 'relu']\n",
    "        }\n",
    "        return self.random_CV(mlp_clf,X,y,param_dist,6,3)\n",
    "    \n",
    "\n",
    "    def train_clf(self,clf,params,X,y):\n",
    "        clf.set_params(**params)\n",
    "        clf.fit(X,y)\n",
    "        print(\" \")\n",
    "        print(\"**Complete Training Accuracy Score**\")\n",
    "        print(clf.score(X,y))\n",
    "        return clf\n",
    "    \n",
    "    def start(self):\n",
    "        print(\"******Classification of Credit card Default Data Set Begins ******\")\n",
    "        file = 'default_of_credit_card_clients.csv'\n",
    "        f = open(file,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        print(\"READING TRAIN DATA\")\n",
    "        \n",
    "       #X_string  = np.char.strip(np.genfromtxt(c,dtype='str'),skip_header=3, delimiter = \",\")#,delimiter = ',',usecols = (1,3,5,6,7,13,14)))\n",
    "        \n",
    "        X_float = np.loadtxt(file,delimiter = \",\", dtype = np.float,skiprows=2).astype(int)\n",
    "       \n",
    "        \n",
    "        X = X_float[:,1:-1] #seperating serial Id and Label columns\n",
    "        y = X_float[:,-1]\n",
    "        \n",
    "       # print(X[0])\n",
    "            \n",
    "        print(y[y==0].shape)\n",
    "        print(y.shape)\n",
    "        \n",
    "        X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n",
    "        \n",
    "        X_raw_train = X_train\n",
    "        \n",
    "        X_train,scaler = self.scale_data(X_train)\n",
    "        \n",
    "        X_train = scaler.transform(X_train)\n",
    "        \n",
    "      #  print(X_train[0:5])\n",
    "        \n",
    "        print(\"-- Training KNN --\")\n",
    "        knn_clf = self.train_clf(KNeighborsClassifier(),self.KNN(X_train,y_train),X_train,y_train)\n",
    "        \n",
    "        print(\"__Training SVM__\")\n",
    "        svm_clf = self.train_clf(svm.SVC(),self.SVM_clf(X_train,y_train),X_train,y_train)\n",
    "        \n",
    "        print(\"__Training DTree__\")\n",
    "        dt_clf = self.train_clf(sklearn.tree.DecisionTreeClassifier(),self.DT_clf(X_raw_train,y_train),X_raw_train,y_train)\n",
    "        \n",
    "        print(\"__Training RForest__\")\n",
    "        rf_clf = self.train_clf(sklearn.ensemble.RandomForestClassifier(),self.RF_clf(X_raw_train,y_train),X_raw_train,y_train)\n",
    "        \n",
    "        print(\"__Training Adaboost__\")\n",
    "        adb_clf = self.train_clf(sklearn.ensemble.AdaBoostClassifier(),self.ADB_clf(X_raw_train,y_train),X_raw_train,y_train)\n",
    "        \n",
    "        print(\"__Training Logistic Reg__\")\n",
    "        lr_clf = self.train_clf(sklearn.linear_model.LogisticRegression(),self.LR_clf(X_train,y_train),X_train,y_train)\n",
    "        \n",
    "        print(\"__Training Naive Bayes__\")\n",
    "        nb_clf = self.train_clf(sklearn.naive_bayes.GaussianNB(),self.NB_clf(X_train,y_train),X_train,y_train)\n",
    "        \n",
    "        print(\"__Training MLP__\")\n",
    "        mlp_clf = self.train_clf(sklearn.neural_network.MLPClassifier(),self.MLP_clf(X_train,y_train),X_train,y_train)\n",
    "        \n",
    "        \n",
    "        X_raw_test = X_test\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        print(\"**Test Data Prediction Begins*\")\n",
    "        \n",
    "        print(\"Testing KNN Classifier\")\n",
    "        print(knn_clf.score(X_test,y_test))\n",
    "        \n",
    "        print(\"Testing SVM Classifier\")\n",
    "        print(svm_clf.score(X_test,y_test))\n",
    "        \n",
    "        print(\"Testing Decision Trees\")\n",
    "        print(dt_clf.score(X_raw_test,y_test)) #using Raw data\n",
    "        \n",
    "        print(\"Testing Random Forests\")\n",
    "        print(rf_clf.score(X_raw_test,y_test))\n",
    "        \n",
    "        print(\"Testing Adaboost\")\n",
    "        print(adb_clf.score(X_raw_test,y_test))\n",
    "        \n",
    "        print(\"Testing Logistic Regression\")\n",
    "        print(lr_clf.score(X_test,y_test))\n",
    "        \n",
    "        print(\"Testing Naive Bayes\")\n",
    "        print(nb_clf.score(X_test,y_test))\n",
    "        \n",
    "       # print(\"Testing Neural Network/MLP\")\n",
    "        #print(mlp_clf.score(X_test_rd,y_test))\n",
    "                                \n",
    "                                 \n",
    "                                 \n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Breast_Cancer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def read_data(self):\n",
    "        X = np.loadtxt('wdbc.data', delimiter=',', usecols=range(2,32))\n",
    "        y = np.loadtxt('wdbc.data', delimiter=',', usecols=[1], dtype= '|S1').astype(str)\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        # B = 0, M = 1\n",
    "        y = le.fit_transform(y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def preprocessing(self, X_train, X_test):\n",
    "        # preprocessing using standard scaler\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        return X_train, X_test\n",
    "        \n",
    "    def cv_SVM(self, X, y):\n",
    "        C_grid = [0.1, 1, 10]\n",
    "        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "        svm = sklearn.svm.SVC(kernel='poly')\n",
    "        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid, \"kernel\" : ['poly', 'rbf', 'sigmoid'], }\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(svm, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))        \n",
    "        return gridcv.best_estimator_\n",
    "\n",
    "    def cv_Knn(self, X, y):\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            \"n_neighbors\" : np.arange(1,20),\n",
    "            \"algorithm\" : ['ball_tree', 'kd_tree', 'brute'],\n",
    "            \"weights\" : ['uniform', 'distance'],\n",
    "            \"leaf_size\" : np.arange(1,60)\n",
    "        }\n",
    "        return Breast_Cancer.randomCV(neigh, X, y, param_grid, 400, 6)\n",
    "    \n",
    "    def cv_DT(self, X, y):\n",
    "        dt = tree.DecisionTreeClassifier()\n",
    "        param_grid = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            \"class_weight\" : [None, 'balanced'],\n",
    "            \"presort\" : [True, False],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "            'max_depth': range(1, 20)\n",
    "        }\n",
    "        return Breast_Cancer.randomCV(dt, X, y, param_grid, 400, 6)\n",
    "        \n",
    "    def cv_RandomForest(self, X, y):\n",
    "        rf = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            #\"class_weight\" : [None, 'balanced'],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "            #\"bootstrap\" : [True, False],\n",
    "            #\"oob_score\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "        }\n",
    "        return Breast_Cancer.randomCV(rf, X, y, param_grid, 40, 6)\n",
    "        \n",
    "        \n",
    "    def cv_adaBoost(self, X, y):\n",
    "        ada_boost = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "        param_grid = {'n_estimators': range(1, 100)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(ada_boost, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X, y)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "        \n",
    "    def cv_logReg(self, X, y):\n",
    "        lr = LogisticRegression()\n",
    "        param_grid = {\n",
    "            \"penalty\" : ['l1', 'l2'],\n",
    "            #\"dual\" : [True, False],\n",
    "            \"C\" : np.random.rand(60),\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "            \"multi_class\" : ['ovr', 'auto'],\n",
    "            \"solver\" : [ 'liblinear']\n",
    "        }\n",
    "        return Breast_Cancer.randomCV(lr, X, y, param_grid, 400, 6)\n",
    "        \n",
    "    def cv_GNB(self, X, y):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid = {\n",
    "            \"var_smoothing\" : np.random.random_sample((100,))\n",
    "        }\n",
    "        return Breast_Cancer.randomCV(gnb, X, y, param_grid, 100, 6)\n",
    "    \n",
    "    def cv_NN(self, X, y):\n",
    "        nn = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                                           solver='sgd', batch_size=100, max_iter=10,\n",
    "                                           learning_rate_init=.01, momentum=0.9, alpha=0.05,\n",
    "                                           verbose=False, random_state=0)\n",
    "\n",
    "        param_grid ={\n",
    "                    'hidden_layer_sizes' : range(2,100),\n",
    "                    \"activation\" : ['identity', 'logistic', 'tanh', 'relu']\n",
    "                    }\n",
    "        return Breast_Cancer.randomCV(nn, X, y, param_grid, 200, 6)\n",
    "        \n",
    "    def randomCV(clf, X, y, param_grid, n_iter, cv):\n",
    "        random_search = RandomizedSearchCV(clf, param_distributions = param_grid, n_iter = n_iter, cv = cv, iid = False)\n",
    "        random_search.fit(X, y)\n",
    "        Breast_Cancer.report(random_search.cv_results_)\n",
    "        return random_search.best_estimator_\n",
    "    \n",
    "    def report(results, n_top=3):\n",
    "        for i in range(1, n_top + 1):\n",
    "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "            k = 0 \n",
    "            for candidate in candidates:\n",
    "                print(\"Model with rank: {0}\".format(i))\n",
    "                print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_test_score'][candidate],\n",
    "                      results['std_test_score'][candidate]))\n",
    "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "                print(\"\")\n",
    "                k += 1\n",
    "                if k ==3:\n",
    "                    break\n",
    "    def predict(self, model, X_test, y_test):        \n",
    "        predict = model.predict(X_test)\n",
    "        accuracy = sklearn.metrics.accuracy_score(y_test, predict)\n",
    "        print(accuracy)\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from io import StringIO\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from sklearn.metrics import make_scorer\n",
    "import time\n",
    "\n",
    "\n",
    "class statlog_aus:\n",
    "    def read(self, a):\n",
    "        f = open(a,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        return np.loadtxt(c)\n",
    "    def report(self, results, n_top=3):\n",
    "        for i in range(1, n_top + 1):\n",
    "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "            j = 0\n",
    "            for candidate in candidates:\n",
    "                print(\"Model with rank: {0}\".format(i))\n",
    "                print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_test_score'][candidate],\n",
    "                      results['std_test_score'][candidate]))\n",
    "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "                print(\"\")\n",
    "                if j > 1:\n",
    "                    break\n",
    "                j+=1\n",
    "\n",
    "\n",
    "    def randomCV(self, clf, X, y, param_grid, n_iter, cv):\n",
    "        random_search = RandomizedSearchCV(clf, param_distributions = param_grid,\n",
    "                        n_iter = n_iter, cv = cv, iid = False, n_jobs = -1)\n",
    "        random_search.fit(X, y)\n",
    "        self.report(random_search.cv_results_)\n",
    "        return random_search.best_params_\t\t\n",
    "\n",
    "    def KNN(self, X, y):\n",
    "\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            \"n_neighbors\" : np.arange(1,20),\n",
    "            \"algorithm\" : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            \"weights\" : ['uniform', 'distance'],\n",
    "            \"leaf_size\" : np.arange(1,60)\n",
    "        }\n",
    "        return self.randomCV(neigh, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def SVM(self, X, y):\n",
    "\n",
    "    #        C_grid = [0.1, 1, 10]\n",
    "    #        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "    #        svm_C = svm.SVC(kernel='poly')\n",
    "    #        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid, \"kernel\" : ['poly', 'rbf', 'sigmoid'], }\n",
    "    #        gridcv = GridSearchCV(svm_C, param_grid, verbose=1, cv=3)\n",
    "    #        gridcv.fit(X, y)\n",
    "    #        print(\"best parameters:\", gridcv.best_params_)\n",
    "    #        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "\n",
    "        svm_C = svm.SVC()\n",
    "        param_grid = {\n",
    "            \"kernel\" : ['linear', 'rbf', 'sigmoid'],\n",
    "            \"gamma\" : ['scale', 'auto'],\n",
    "            \"degree\" : np.arange(10),\n",
    "            \"coef0\" : np.random.rand(60)*10,\n",
    "            \"shrinking\" : [False, True],\n",
    "            \"decision_function_shape\" : ['ovo','ovr']\n",
    "        }\n",
    "        return self.randomCV(svm_C, X, y, param_grid, 4, 6)\n",
    "\n",
    "    def DT(self, X, y):\n",
    "        dt = DecisionTreeClassifier()\n",
    "        param_grid = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            \"class_weight\" : [None, 'balanced'],\n",
    "            \"presort\" : [True, False],\n",
    "            \"min_samples_leaf\" : np.arange(1,6)\n",
    "        }\n",
    "        return self.randomCV(dt, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def RF(self, X, y):\n",
    "        rf = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "    #\t\t\"class_weight\" : [None, 'balanced'],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "    #\t\t\"bootstrap\" : [True, False],\n",
    "    #\t\t\"oob_score\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "        }\n",
    "        return self.randomCV(rf, X, y, param_grid, 40, 6)\n",
    "\n",
    "    def Ada(self, X, y):\n",
    "        ada = AdaBoostClassifier(algorithm = \"SAMME\")\n",
    "        param_grid = {\n",
    "    #\t\t\"base_estimator\" : ['classes', 'n_classes_', None],\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)]\n",
    "    #\t\t\"learning_rate\" : [10*x for x in np.random.random_sample((100,))]\n",
    "    #\t\t\"algorithm\" : ['SAMME']\n",
    "        }\n",
    "        return self.randomCV(ada, X, y, param_grid, 40, 6)\n",
    "\n",
    "    def LR(self, X, y):\n",
    "        lr = LogisticRegression()\n",
    "        param_grid = {\n",
    "            \"penalty\" : ['l1', 'l2'],\n",
    "    #\t\t\"dual\" : [True, False],\n",
    "            \"C\" : np.random.rand(60),\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "            \"multi_class\" : ['ovr', 'auto'],\n",
    "            \"solver\" : [ 'liblinear']\n",
    "        }\n",
    "        return self.randomCV(lr, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def GNB(self, X, y):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid = {\n",
    "            \"var_smoothing\" : np.random.random_sample((100,))\n",
    "        }\n",
    "        return self.randomCV(gnb, X, y, param_grid, 100, 6)\n",
    "\n",
    "    def NN(self, X, y):\n",
    "        nn = MLPClassifier()\n",
    "        param_grid = {\n",
    "            \"hidden_layer_sizes\" : np.arange(2,200),\n",
    "            \"activation\" : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            \"solver\" : ['lbfgs', 'sgd', 'adam'],\n",
    "    #\t\t\"verbose\" : [True, False],\n",
    "            \"warm_start\" : [False, True]\n",
    "        }\n",
    "        return self.randomCV(nn, X, y, param_grid, 200, 6)\n",
    "    def start(self):\n",
    "        data = self.read('australian.dat')\n",
    "        y = data[:,-1]\n",
    "        x = data[:,0:data.shape[1] -1]\n",
    "\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "        x_test, x_train = np.split(x, [105])\n",
    "        y_test, y_train = np.split(y, [105])\n",
    "\n",
    "        scaler = StandardScaler()                         # scaling features\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        \n",
    "        # --------------------> KNN\n",
    "\n",
    "        param = self.KNN(x_train,y_train)\n",
    "        knn_c = KNeighborsClassifier().set_params(**param)\n",
    "        knn_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",knn_c.score(x_test, y_test))\n",
    "        \n",
    "        # --------------------> Decision Tree\n",
    "\n",
    "        param = self.DT(x_train,y_train)\n",
    "        dt_c = DecisionTreeClassifier().set_params(**param)\n",
    "        dt_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",dt_c.score(x_test, y_test))\n",
    "        \n",
    "        # --------------------> Random Forest\n",
    "\n",
    "        param = self.RF(x_train,y_train)\n",
    "        rf_c = RandomForestClassifier().set_params(**param)\n",
    "        rf_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",rf_c.score(x_test, y_test))\n",
    "        \n",
    "        # --------------------> Adaboost\n",
    "\n",
    "        param = self.Ada(x_train,y_train)\n",
    "        ada_c = AdaBoostClassifier().set_params(**param)\n",
    "        ada_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",ada_c.score(x_test, y_test))\n",
    "        \n",
    "        # ---------------------> Logistic regression\n",
    "\n",
    "        param = self.LR(x_train,y_train)\n",
    "        lr_c = LogisticRegression().set_params(**param)\n",
    "        lr_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",lr_c.score(x_test, y_test))\n",
    "        \n",
    "        # ---------------------> Gaussian NB\n",
    "\n",
    "        param = self.GNB(x_train,y_train)\n",
    "        gnb_c = GaussianNB().set_params(**param)\n",
    "        gnb_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",gnb_c.score(x_test, y_test))\n",
    "        \n",
    "        # ---------------------> Neural Network\n",
    "\n",
    "        param = self.NN(x_train,y_train)\n",
    "        nn_c = MLPClassifier().set_params(**param)\n",
    "        nn_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",nn_c.score(x_test, y_test))\n",
    "        \n",
    "        ## --------------------> SVM\n",
    "\n",
    "        param = self.SVM(x_train,y_train)\n",
    "        svm_c = svm.SVC().set_params(**param)\n",
    "        svm_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",svm_c.score(x_test, y_test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class German_CC:\n",
    "        \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def read_data(self):\n",
    "        # reading data and doing 80-20 split\n",
    "        data = np.loadtxt('german.data-numeric')\n",
    "        X = data[:,:-1]\n",
    "        y = data[:,-1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "        \n",
    "    def cv_SVM(self, X, y):\n",
    "        scorer = make_scorer(precision_score)\n",
    "        C_grid = [0.1, 1, 10]\n",
    "        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "        svm = sklearn.svm.SVC(kernel='rbf')\n",
    "        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid, 'kernel' : ['rbf', 'sigmoid']}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(svm, param_grid, n_jobs=-1, verbose=1, cv=3, scoring = scorer)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "\n",
    "    def cv_Knn(self, X, y):\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            \"n_neighbors\" : np.arange(1,20),\n",
    "            \"algorithm\" : ['ball_tree', 'kd_tree', 'brute'],\n",
    "            \"weights\" : ['uniform', 'distance'],\n",
    "            \"leaf_size\" : np.arange(1,60)\n",
    "        }\n",
    "        return German_CC.randomCV(neigh, X, y, param_grid, 400, 6)\n",
    "    \n",
    "    def cv_DT(self, X, y):        \n",
    "        dt = tree.DecisionTreeClassifier()\n",
    "        param_grid = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            \"class_weight\" : [None, 'balanced'],\n",
    "            \"presort\" : [True, False],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "            'max_depth': range(1, 20)\n",
    "        }\n",
    "        return German_CC.randomCV(dt, X, y, param_grid, 400, 6)\n",
    "        \n",
    "    def cv_RandomForest(self, X, y):\n",
    "        rf = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            #\"class_weight\" : [None, 'balanced'],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "            #\"bootstrap\" : [True, False],\n",
    "            #\"oob_score\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "        }\n",
    "        return German_CC.randomCV(rf, X, y, param_grid, 40, 6)\n",
    "        \n",
    "    def cv_adaBoost(self, X, y):\n",
    "        scorer = make_scorer(precision_score)\n",
    "        ada_boost = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "        param_grid = {'n_estimators': range(1, 100)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(ada_boost, param_grid, verbose=1, cv=3, scoring=scorer)\n",
    "        gridcv.fit(X, y)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "    \n",
    "    def cv_logReg(self, X, y):\n",
    "        lr = LogisticRegression()\n",
    "        param_grid = {\n",
    "            \"penalty\" : ['l1', 'l2'],\n",
    "            #\"dual\" : [True, False],\n",
    "            \"C\" : np.random.rand(60),\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "            \"multi_class\" : ['ovr', 'auto'],\n",
    "            \"solver\" : [ 'liblinear']\n",
    "        }\n",
    "        return German_CC.randomCV(lr, X, y, param_grid, 400, 6)\n",
    "        \n",
    "    def cv_GNB(self, X, y):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid = {\n",
    "            \"var_smoothing\" : np.random.random_sample((100,))\n",
    "        }\n",
    "        return German_CC.randomCV(gnb, X, y, param_grid, 100, 6)\n",
    "    \n",
    "    def cv_NN(self, X, y):\n",
    "        nn = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                                           solver='sgd', batch_size=100, max_iter=10,\n",
    "                                           learning_rate_init=.01, momentum=0.9, alpha=0.05,\n",
    "                                           verbose=False, random_state=0)\n",
    "\n",
    "        param_grid ={\n",
    "                    'hidden_layer_sizes' : range(2,100),\n",
    "                    \"activation\" : ['identity', 'logistic', 'tanh', 'relu']\n",
    "                    }\n",
    "        return German_CC.randomCV(nn, X, y, param_grid, 200, 6)\n",
    "        \n",
    "    def randomCV(clf, X, y, param_grid, n_iter, cv):\n",
    "        scorer = make_scorer(precision_score)\n",
    "        random_search = RandomizedSearchCV(clf, param_distributions = param_grid, n_iter = n_iter, cv = cv, iid = False, \n",
    "                                           scoring = scorer)\n",
    "        random_search.fit(X, y)        \n",
    "        German_CC.report(random_search.cv_results_)\n",
    "        return random_search.best_estimator_\n",
    "    \n",
    "    def report(results, n_top=1):\n",
    "        for i in range(1, n_top + 1):\n",
    "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "            k = 0\n",
    "            for candidate in candidates:                \n",
    "                print(\"Model with rank: {0}\".format(i))\n",
    "                print(\"Mean precision score on validation data: {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_test_score'][candidate],\n",
    "                      results['std_test_score'][candidate]))\n",
    "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "                print(\"\")\n",
    "                k += 1\n",
    "                if k == 3:\n",
    "                    break\n",
    "                \n",
    "    def predict(self, model, X_test, y_test):\n",
    "        predict = model.predict(X_test)\n",
    "        accuracy = sklearn.metrics.accuracy_score(y_test, predict)\n",
    "        print(\"Final Accuracy on test data : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class steel:\n",
    "    def read(self,a):\n",
    "        f = open(a,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        return np.loadtxt(c)\n",
    "\n",
    "    def report(self, results, n_top=3):\n",
    "        for i in range(1, n_top + 1):\n",
    "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "            j = 0\n",
    "            print(\"\\n\\n\\n\\n\\n\")\n",
    "            for candidate in candidates:\n",
    "                print(\"Model with rank: {0}\".format(i))\n",
    "                print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_test_score'][candidate],\n",
    "                      results['std_test_score'][candidate]))\n",
    "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "                print(\"\")\n",
    "                if j > 1:\n",
    "                    break\n",
    "                j+=1\n",
    "\n",
    "\n",
    "    def randomCV(self, clf, X, y, param_grid, n_iter, cv):\n",
    "        random_search = RandomizedSearchCV(clf, param_distributions = param_grid,\n",
    "                        n_iter = n_iter, cv = cv, iid = False, n_jobs = -1)\n",
    "        random_search.fit(X, y)\n",
    "        self.report(random_search.cv_results_)\n",
    "        return random_search.best_params_\t\t\n",
    "\n",
    "    def KNN(self, X, y):\n",
    "\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            \"n_neighbors\" : np.arange(1,20),\n",
    "            \"algorithm\" : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            \"weights\" : ['uniform', 'distance'],\n",
    "            \"leaf_size\" : np.arange(1,60)\n",
    "        }\n",
    "        return self.randomCV(neigh, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def SVM(self, X, y):\n",
    "\n",
    "    #        C_grid = [0.1, 1, 10]\n",
    "    #        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "    #        svm_C = svm.SVC(kernel='poly')\n",
    "    #        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid, \"kernel\" : ['poly', 'rbf', 'sigmoid'], }\n",
    "    #        gridcv = GridSearchCV(svm_C, param_grid, verbose=1, cv=3)\n",
    "    #        gridcv.fit(X, y)\n",
    "    #        print(\"best parameters:\", gridcv.best_params_)\n",
    "    #        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "\n",
    "        svm_C = svm.SVC()\n",
    "        param_grid = {\n",
    "            \"kernel\" : ['linear', 'rbf', 'sigmoid'],\n",
    "            \"gamma\" : ['scale', 'auto'],\n",
    "            \"degree\" : np.arange(10),\n",
    "            \"coef0\" : np.random.rand(60)*10,\n",
    "            \"shrinking\" : [False, True],\n",
    "            \"decision_function_shape\" : ['ovo','ovr']\n",
    "        }\n",
    "        return self.randomCV(svm_C, X, y, param_grid, 50, 6)\n",
    "\n",
    "    def DT(self, X, y):\n",
    "        dt = DecisionTreeClassifier()\n",
    "        param_grid = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            \"class_weight\" : [None, 'balanced'],\n",
    "            \"presort\" : [True, False],\n",
    "            \"min_samples_leaf\" : np.arange(1,6)\n",
    "        }\n",
    "        return self.randomCV(dt, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def RF(self, X, y):\n",
    "        rf = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "    #\t\t\"class_weight\" : [None, 'balanced'],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "    #\t\t\"bootstrap\" : [True, False],\n",
    "    #\t\t\"oob_score\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "        }\n",
    "        return self.randomCV(rf, X, y, param_grid, 40, 6)\n",
    "\n",
    "    def Ada(self, X, y):\n",
    "        ada = AdaBoostClassifier(algorithm = \"SAMME\")\n",
    "        param_grid = {\n",
    "    #\t\t\"base_estimator\" : ['classes', 'n_classes_', None],\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)]\n",
    "    #\t\t\"learning_rate\" : [10*x for x in np.random.random_sample((100,))]\n",
    "    #\t\t\"algorithm\" : ['SAMME']\n",
    "        }\n",
    "        return self.randomCV(ada, X, y, param_grid, 40, 6)\n",
    "\n",
    "    def LR(self, X, y):\n",
    "        lr = LogisticRegression()\n",
    "        param_grid = {\n",
    "            \"penalty\" : ['l1', 'l2'],\n",
    "    #\t\t\"dual\" : [True, False],\n",
    "            \"C\" : np.random.rand(60),\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "            \"multi_class\" : ['ovr', 'auto'],\n",
    "            \"solver\" : [ 'liblinear']\n",
    "        }\n",
    "        return self.randomCV(lr, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def GNB(self, X, y):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid = {\n",
    "            \"var_smoothing\" : np.random.random_sample((100,))\n",
    "        }\n",
    "        return self.randomCV(gnb, X, y, param_grid, 100, 6)\n",
    "\n",
    "    def NN(self, X, y):\n",
    "        nn = MLPClassifier()\n",
    "        param_grid = {\n",
    "            \"hidden_layer_sizes\" : np.arange(2,200),\n",
    "            \"activation\" : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            \"solver\" : ['lbfgs', 'sgd', 'adam'],\n",
    "    #\t\t\"verbose\" : [True, False],\n",
    "            \"warm_start\" : [False, True]\n",
    "        }\n",
    "        return self.randomCV(nn, X, y, param_grid, 200, 6)\n",
    "\n",
    "    def start(self):\n",
    "        data = self.read('Faults.NNA')\n",
    "#         print(data.shape)\n",
    "\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "        y = data[:,-7:]\n",
    "\n",
    "        x = data[:,0:data.shape[1] -7]\n",
    "        #         y_pred = y\n",
    "#         print(y)\n",
    "\n",
    "        y_pred = np.zeros((y.shape[0]))\n",
    "        for i in range(y.shape[0]):\n",
    "            y_pred[i] = np.where(y[i] == 1)[0][0]\n",
    "\n",
    "        y = y_pred\n",
    "        \n",
    "        x_test, x_train = np.split(x, [290])\n",
    "        y_test, y_train = np.split(y, [290])\n",
    "\n",
    "        scaler = StandardScaler()                         # scaling features\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        \n",
    "        # --------------------> KNN\n",
    "\n",
    "        param = self.KNN(x_train,y_train)\n",
    "        knn_c = KNeighborsClassifier().set_params(**param)\n",
    "        knn_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",knn_c.score(x_test, y_test))        \n",
    "        \n",
    "        # --------------------> Decision Tree\n",
    "\n",
    "        param = self.DT(x_train,y_train)\n",
    "        dt_c = DecisionTreeClassifier().set_params(**param)\n",
    "        dt_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",dt_c.score(x_test, y_test))        \n",
    "        \n",
    "        # --------------------> Random Forest\n",
    "\n",
    "        param = self.RF(x_train,y_train)\n",
    "        rf_c = RandomForestClassifier().set_params(**param)\n",
    "        rf_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",rf_c.score(x_test, y_test))\n",
    "        \n",
    "        # --------------------> Adaboost\n",
    "\n",
    "        param = self.Ada(x_train,y_train)\n",
    "        ada_c = AdaBoostClassifier().set_params(**param)\n",
    "        ada_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",ada_c.score(x_test, y_test))\n",
    "        \n",
    "        # ---------------------> Logistic regression\n",
    "\n",
    "        param = self.LR(x_train,y_train)\n",
    "        lr_c = LogisticRegression().set_params(**param)\n",
    "        lr_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",lr_c.score(x_test, y_test))\n",
    "        \n",
    "        # ---------------------> Gaussian NB\n",
    "\n",
    "        param = self.GNB(x_train,y_train)\n",
    "        gnb_c = GaussianNB().set_params(**param)\n",
    "        gnb_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",gnb_c.score(x_test, y_test))\n",
    "        \n",
    "        ################### both of the classifiers below fail to converge #####################\n",
    "        \n",
    "        # ---------------------> Neural Network\n",
    "\n",
    "#        This does converge and gives good result but takes a long time\n",
    "        \n",
    "#         param = self.NN(x_train,y_train)\n",
    "#         nn_c = MLPClassifier().set_params(**param)\n",
    "#         nn_c.fit(x_train, y_train)\n",
    "\n",
    "#         print(\"Score with test data\",nn_c.score(x_test, y_test))\n",
    "\n",
    "#         ## --------------------> SVM\n",
    "\n",
    "#         param = self.SVM(x_train,y_train)\n",
    "#         svm_c = svm.SVC().set_params(**param)\n",
    "#         svm_c.fit(x_train, y_train)\n",
    "\n",
    "#         print(\"Score with test data\",svm_c.score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from io import StringIO\n",
    "import scipy\n",
    "import scipy.stats               # For reciprocal distribution\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import svm\n",
    "import sklearn.tree        # For DecisionTreeClassifier class\n",
    "import sklearn.ensemble    # For RandomForestClassifier class\n",
    "import sklearn.linear_model # For Logistic Classifier\n",
    "#from sklearn.neighbors import LSHForest\n",
    "import sklearn.naive_bayes #For Naive Bayes\n",
    "import sklearn.neural_network #For MLP classifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # Ignore sklearn deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)       # Ignore sklearn deprecation warnings\n",
    "np.set_printoptions(precision=20, suppress=True)\n",
    "\n",
    "class Adult:\n",
    "    #Removing Race and Sex Variable form data for fairness purpose\n",
    "    def preprocess_data_OH(self,X,X_num,y):\n",
    "        X= X.tolist()\n",
    "        #print(X[0])\n",
    "        workclass = ['Private','Self-emp-not-inc','Self-emp-inc','Federal-gov','Local-gov','State-gov','Without-pay','Never-worked']\n",
    "        education = ['Bachelors','Some-college','11th','HS-grad','Prof-school','Assoc-acdm','Assoc-voc','9th','7th-8th','12th','Masters','1st-4th','10th','Doctorate','5th-6th','Preschool']\n",
    "        marital_status = ['Married-civ-spouse','Divorced','Never-married','Separated','Widowed','Married-spouse-absent','Married-AF-spouse']\n",
    "        occupation = ['Tech-support','Craft-repair','Other-service','Sales','Exec-managerial','Prof-specialty','Handlers-cleaners','Machine-op-inspct','Adm-clerical','Farming-fishing','Transport-moving','Priv-house-serv','Protective-serv','Armed-Forces']\n",
    "        relationship = ['Wife','Own-child','Husband','Not-in-family','Other-relative','Unmarried']\n",
    "        native_country = ['United-States','Cambodia','England','Puerto-Rico','Canada','Germany','Outlying-US(Guam-USVI-etc)','India','Japan','Greece','South','China','Cuba','Iran','Honduras','Philippines','Italy','Poland','Jamaica','Vietnam','Mexico','Portugal','Ireland','France','Dominican-Republic','Laos','Ecuador','Taiwan','Haiti','Columbia','Hungary','Guatemala','Nicaragua','Scotland','Thailand','Yugoslavia','El-Salvador','Trinadad&Tobago','Peru','Hong','Holand-Netherlands']\n",
    "        \n",
    "        encoder = preprocessing.OneHotEncoder(categories=[workclass, education, marital_status,occupation,relationship,native_country],\n",
    "                                              handle_unknown='ignore',sparse=False)\n",
    "        print(encoder.fit(X))\n",
    "        X = encoder.transform(X)\n",
    "        X = np.column_stack((X_num,X)) #Combine numeric and encoded string input data\n",
    "        y[y=='<=50K']= 0\n",
    "        y[y=='>50K'] = 1\n",
    "        y[y=='<=50K.']= 0\n",
    "        y[y=='>50K.'] = 1\n",
    "        y = y.astype(int)\n",
    "        return (X,y)\n",
    "    \n",
    "    def scale_data(self,X):\n",
    "        scaler = preprocessing.StandardScaler(with_mean = False).fit(X)\n",
    "        X = scaler.transform(X)\n",
    "        return(X,scaler)\n",
    "    \n",
    "    def preprocess_data_OE(self,X,X_num,y):\n",
    "        X = X.tolist()\n",
    "        encoder = preprocessing.OrdinalEncoder()\n",
    "        encoder.fit(X)\n",
    "        X = encoder.transform(X)\n",
    "        X = np.column_stack((X_num,X))\n",
    "        y[y=='<=50K.']= 0\n",
    "        y[y=='<=50K.']= 0\n",
    "        y[y=='>50K.'] = 1\n",
    "        y[y=='<=50K.']= 0\n",
    "        y = y.astype(int)\n",
    "        return(X,y)\n",
    "    \n",
    "    def __init__(self):\n",
    "        #write your actual run code\n",
    "        pass\n",
    "    def random_CV(self,clf,X,y,param_grid,n_iter,cv):\n",
    "        scorer_AUROC = make_scorer(sklearn.metrics.roc_auc_score) #using ROC for scoring criteria\n",
    "        print(\"Starting search\")\n",
    "        print(\"Score mechanism implemented by RandomizedCV - AUROC score\")\n",
    "        random_search = model_selection.RandomizedSearchCV(clf, param_distributions = param_grid,n_iter = n_iter, cv = cv,\n",
    "                                           iid = False,verbose=1,scoring = scorer_AUROC,n_jobs = 4)\n",
    "        random_search.fit(X, y)\n",
    "        print(\"best parameters:\", random_search.best_params_)\n",
    "        print(\"%.1f%% accuracy -AUROC on validation sets (average)\" % (random_search.best_score_*100))\n",
    "        return random_search.best_params_\n",
    "    \n",
    "    def KNN(self,X,y):\n",
    "        print(\"Starting KNN classification- Expected to take about 5 mins as its a hude data set\")\n",
    "       \n",
    "       # X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n",
    "        knn_clf = KNeighborsClassifier()\n",
    "        param_dist = {'n_neighbors': range(1,100),\n",
    "                     \"algorithm\" : ['ball_tree', 'kd_tree'],\n",
    "                    \"weights\" : ['uniform', 'distance'],\n",
    "                    \"leaf_size\" : range(1,100)}\n",
    "        print(\"Calling random_cv\")\n",
    "        return self.random_CV(knn_clf,X,y,param_dist,4,3)\n",
    "        \n",
    "        \n",
    "    def SVM_clf(self,X,y):\n",
    "        print(\"Starting SVM classification\")\n",
    "        #svm_clf = svm.SVC()\n",
    "        svm_clf = svm.LinearSVC()\n",
    "        param_dist = {\n",
    "            'C'     : scipy.stats.reciprocal(1.0, 1000.),\n",
    "            #'penalty': ['l1','l2'],\n",
    "            'dual': [True,False],\n",
    "            'max_iter' : np.arange(2000,10000,200)\n",
    "            }\n",
    "        return self.random_CV(svm_clf,X,y,param_dist,8,3)\n",
    "        \n",
    "    def DT_clf(self,X,y):\n",
    "        tree_clf = sklearn.tree.DecisionTreeClassifier()\n",
    "        param_dist = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"max_depth\" :[None,500,750,1000,1500,2000],\n",
    "            \"max_features\": [\"sqrt\",\"log2\",None]\n",
    "        }\n",
    "        return self.random_CV(tree_clf,X,y,param_dist,15,5)\n",
    "    \n",
    "    def RF_clf(self,X,y):\n",
    "        print(\"Random Forest Classifier Called\")\n",
    "        rf_clf = sklearn.ensemble.RandomForestClassifier()\n",
    "        param_dist = {\n",
    "            \"n_estimators\" : [10,25,50,75,100,125,150,175,200],\n",
    "            \"max_depth\" :[None,500,750,1000,1500,2000],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"max_features\": [\"sqrt\",\"log2\",None],\n",
    "            \"n_jobs\": [4],\n",
    "            \"warm_start\" : [True]\n",
    "           # \"bootstrap\": [True,False]\n",
    "        }\n",
    "        return self.random_CV(rf_clf,X,y,param_dist,12,3)\n",
    "        \n",
    "        \n",
    "    def ADB_clf(self,X,y):\n",
    "        print(\"AdaBoost Classifier Called\")\n",
    "        adb_clf = sklearn.ensemble.AdaBoostClassifier()\n",
    "        param_dist = {\n",
    "            \"n_estimators\" : [50,100,150,200,300,500,750,1000],\n",
    "            \"algorithm\" : ['SAMME', 'SAMME.R'],\n",
    "            \"random_state\" : [0]\n",
    "        }\n",
    "        return self.random_CV(adb_clf,X,y,param_dist,5,5)\n",
    "        \n",
    "    \n",
    "    def LR_clf(self,X,y):\n",
    "        print(\"Logistic Reg Classifier Called\")\n",
    "        lr_clf = sklearn.linear_model.LogisticRegression()\n",
    "        param_dist = {\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            'C'     : scipy.stats.reciprocal(1.0, 1000.),\n",
    "            \"solver\" : ['lbfgs','sag','saga','newton-cg'],\n",
    "            \"penalty\" : ['l2'],\n",
    "            'max_iter' : np.arange(500,10000,500)\n",
    "        }\n",
    "        return self.random_CV(lr_clf,X,y,param_dist,12,3)\n",
    "        \n",
    "    \n",
    "    def NB_clf(self,X,y):\n",
    "        print(\"Naive Bayes Classifier called\")\n",
    "        nb_clf = sklearn.naive_bayes.GaussianNB()\n",
    "        param_dist = {\n",
    "            \"priors\": [None,[0.5,0.5],[0.6,0.4],[0.4,0.6],[0.3,0.7],[0.7,0.3]]\n",
    "        }\n",
    "        return self.random_CV(nb_clf,X,y,param_dist,5,5)\n",
    "\n",
    "    \n",
    "    def MLP_clf(self,X,y):\n",
    "        print(\"Neural Network / MLP Classifier called\")\n",
    "        mlp_clf = sklearn.neural_network.MLPClassifier()\n",
    "        param_dist = {\n",
    "            \"hidden_layer_sizes\" : [(100,), (200,),(100,50),(200,50),(200,100),(100,100,50)],\n",
    "            \"solver\" : ['sgd'],\n",
    "            \"learning_rate\" : ['constant','invscaling'],\n",
    "            \"max_iter\" : [200,300,500],\n",
    "            \"warm_start\" : [True],\n",
    "            \"activation\" :['tanh', 'relu']\n",
    "        }\n",
    "        return self.random_CV(mlp_clf,X,y,param_dist,5,3)\n",
    "    \n",
    "    def train_clf(self,clf,params,X,y):\n",
    "        clf.set_params(**params)\n",
    "        clf.fit(X,y)\n",
    "        print(\"Complete Training Accuracy\")\n",
    "        print(clf.score(X,y))\n",
    "        return clf\n",
    "        \n",
    "        \n",
    "    def start(self):\n",
    "        print(\"******Classification of Adult Data Set Begins ******\")\n",
    "        file = 'adult.data'\n",
    "        f = open(file,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        \n",
    "        # *****\n",
    "        #*****Note : There is a seperate file for testing data\n",
    "        #******\n",
    "        \n",
    "        print(\"READING TRAIN DATA\")\n",
    "        # Ignoring Race and Sex Attributes\n",
    "        \n",
    "        #X_inp = np.loadtxt(c, delimiter = \",\",usecols =(0,1,2,3,4,5,6,7,10,11,12,13,14),dtype = {'names':('age','workclass','fnlwgt',\n",
    "        #                                                                                                    'education','education-num',\n",
    "        #                                                                                                    'marital-status','occupation',\n",
    "        #                                                                                                   'relationship','capital-gain',\n",
    "        #                                                                                                    'capital-loss','hours-per-week',\n",
    "        #                                                                                                    'native-country'),\n",
    "        #                                                                                           'formats':(np.float,'|S25',np.float,\n",
    "         #                                                                                                    '|S25', np.float, '|S25',\n",
    "         #                                                                                                    '|S25', '|S25', np.float,\n",
    "          #                                                                                                   np.float,np.float,'|S25')})\n",
    "        X_string  = np.char.strip(np.genfromtxt(c,dtype='str',delimiter = ',',usecols = (1,3,5,6,7,13,14)))\n",
    "        X_float = np.loadtxt(file,delimiter = \",\",usecols = (0,2,4,10,11,12), dtype = np.float).astype(int)\n",
    "        \n",
    "        print(\"Pre Processing Train data with One Hot Encoding\")\n",
    "        (X,y) = self.preprocess_data_OH(X_string[:,:-1],X_float,X_string[:,-1])\n",
    "        \n",
    "        \n",
    "        print(\"Normalizing data with Standard Scaler\")\n",
    "        (X,scaler) = self.scale_data(X)\n",
    "        \n",
    "        #Reducing dimensions to consider first 50 Principle Components based on explained_variance_ratio scores\n",
    "        print(\"Reducing dimensionality to improve classifier run time\")\n",
    "        pca = PCA(n_components = 50)\n",
    "        pca.fit(X)\n",
    "        X_rd = pca.fit_transform(X)\n",
    "        \n",
    "        \n",
    "        #print(X_rd[0:5])\n",
    "        #print(pca.explained_variance_ratio_)\n",
    "        \n",
    "        print(\"-- Training KNN --\")\n",
    "        knn_clf = self.train_clf(KNeighborsClassifier(),self.KNN(X_rd,y),X_rd,y) #used PCA reduced data\n",
    "        \n",
    "        #Calling SVM Classifier using unreduced data dimensions\n",
    "        print(\"--Training SVM \")\n",
    "        svm_clf = self.train_clf(svm.LinearSVC(),self.SVM_clf(X,y),X,y)\n",
    "        \n",
    "        print(\" Preprocessing with Ordinal Encoding for Decison Tree Algorithms\")\n",
    "        \n",
    "        (X_oe,y_oe) = self.preprocess_data_OE(X_string[:,:-1],X_float,X_string[:,-1]) # usine Ordinal Encoding for Decision tree algorithms\n",
    "        \n",
    "        print(\"--Training Decision Trees--\")\n",
    "        dt_clf = self.train_clf(sklearn.tree.DecisionTreeClassifier(),self.DT_clf(X_oe,y_oe),X_oe,y_oe)\n",
    "        \n",
    "        print(\"--Training Random Forests--\")\n",
    "        rf_clf = self.train_clf(sklearn.ensemble.RandomForestClassifier(),self.RF_clf(X_oe,y_oe),X_oe,y_oe)\n",
    "        \n",
    "        print(\"--Training AdaBoost-- \")\n",
    "        adb_clf = self.train_clf(sklearn.ensemble.AdaBoostClassifier(),self.ADB_clf(X_oe,y_oe),X_oe,y_oe)\n",
    "        \n",
    "        print(\"--Training Logistic Reg--\")\n",
    "        lr_clf = self.train_clf(sklearn.linear_model.LogisticRegression(),self.LR_clf(X_rd,y),X_rd,y)\n",
    "        \n",
    "        print(\"--Training Naive Bayes-- \")\n",
    "        nb_clf = self.train_clf(sklearn.naive_bayes.GaussianNB(),self.NB_clf(X_rd,y),X_rd,y)\n",
    "        \n",
    "      #  print(\"--Training Neural Networks/MLP--\")\n",
    "        #mlp_clf = self.train_clf(sklearn.neural_network.MLPClassifier(),self.MLP_clf(X_rd,y),X_rd,y)\n",
    "        \n",
    "        print(\"--READING TEST DATA\")\n",
    "        \n",
    "        file = 'adult.test'\n",
    "        f = open(file,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        \n",
    "        \n",
    "        X_string  = np.char.strip(np.genfromtxt(c,dtype='str',delimiter = ',',usecols = (1,3,5,6,7,13,14),skip_header=1))\n",
    "        X_float = np.loadtxt(file,delimiter = \",\",usecols = (0,2,4,10,11,12), dtype = np.float,skiprows=1).astype(int)\n",
    "        \n",
    "        print(\"One Hot Encoding of Data\")\n",
    "        (X_test,y_test) = self.preprocess_data_OH(X_string[:,:-1],X_float,X_string[:,-1])\n",
    "        \n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        (X_test_oe,y_test_oe) = self.preprocess_data_OE(X_string[:,:-1],X_float,X_string[:,-1])\n",
    "        \n",
    "        print(\"Reducing dimensionality to improve classifier run time\")\n",
    "        pca = PCA(n_components = 50)\n",
    "        X_test_rd = pca.fit_transform(X_test)\n",
    "        \n",
    "        print(\"**Test Data Prediction Begins*\")\n",
    "        \n",
    "        print(\"Testing KNN Classifier\")\n",
    "        print(knn_clf.score(X_test_rd,y_test))\n",
    "        \n",
    "        print(\"Testing SVM Classifier\")\n",
    "        print(svm_clf.score(X_test,y_test))\n",
    "        \n",
    "        print(\"Testing Decision Trees\")\n",
    "        print(dt_clf.score(X_test_oe,y_test_oe)) #using ordinal encoding\n",
    "        \n",
    "        print(\"Testing Random Forests\")\n",
    "        print(rf_clf.score(X_test_oe,y_test_oe))\n",
    "        \n",
    "        print(\"Testing Adaboost\")\n",
    "        print(adb_clf.score(X_test_oe,y_test_oe))\n",
    "        \n",
    "        print(\"Testing Logistic Regression\")\n",
    "        print(lr_clf.score(X_test_rd,y_test))\n",
    "        \n",
    "        print(\"Testing Naive Bayes\")\n",
    "        print(nb_clf.score(X_test_rd,y_test))\n",
    "        \n",
    "       # print(\"Testing Neural Network/MLP\")\n",
    "        #print(mlp_clf.score(X_test_rd,y_test))\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class yeast:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def preprocess_data_LB(self,y):\n",
    "        \n",
    "        lb = preprocessing.LabelBinarizer()\n",
    "        lb.fit(y)\n",
    "        print(lb.classes_)\n",
    "        y = lb.transform(y)\n",
    "        return (y,lb)\n",
    "        \n",
    "    def preprocess_data_lbl(self,y):\n",
    "        #modifies label names with int values\n",
    "        \n",
    "        y[y=='CYT'] = 0\n",
    "        y[y=='NUC'] = 1\n",
    "        y[y=='MIT'] = 2\n",
    "        y[y=='ME3'] = 3\n",
    "        y[y=='ME2'] = 4\n",
    "        y[y=='ME1'] = 5\n",
    "        y[y=='EXC'] = 6\n",
    "        y[y=='VAC'] = 7\n",
    "        y[y=='POX'] = 8\n",
    "        y[y=='ERL'] = 9\n",
    "        \n",
    "        return y\n",
    "        \n",
    "        \n",
    "    def scale_data(self,X):\n",
    "        scaler = preprocessing.StandardScaler().fit(X)\n",
    "        X = scaler.transform(X)\n",
    "        return(X,scaler) \n",
    "       \n",
    "    def AUPR_score_func(self,y_true, y_pred):\n",
    "        lb = LabelBinarizer()\n",
    "        lb = lb.fit(np.vstack((y_true,y_pred)))\n",
    "        precision, recall, threshold = sklearn.metrics.precision_recall_curve(lb.transform(y_true), lb.transform(y_pred))\n",
    "        return sklearn.metrics.auc(recall, precision)\n",
    "    \n",
    "    def AVG_prec_score(self,y_true,y_pred):\n",
    "           # lb = LabelBinarizer()\n",
    "            #lb = lb.fit(np.vstack((y_true,y_pred)))\n",
    "            return sklearn.metrics.label_ranking_average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    def random_CV(self,clf,X,y,param_grid,n_iter,cv):\n",
    "        scorer_AUROC = make_scorer(sklearn.metrics.roc_auc_score) #using ROC for scoring criteria\n",
    "        scorer_PRECISON = make_scorer(self.AUPR_score_func) #Check PR curve Area due to imbalanced data set 1:4\n",
    "        scorer_ACCURACY = make_scorer(sklearn.metrics.accuracy_score) #Chec accuracy\n",
    "        scorer_AvgPre = make_scorer(self.AVG_prec_score)\n",
    "        \n",
    "        scoring = { 'Accuracy':scorer_ACCURACY }\n",
    "        \n",
    "        \n",
    "        print(\"Starting search\")\n",
    "        print(\"Score mechanism implemented by RandomizedCV - AUROC score + Accuracy + AUPR\")\n",
    "        random_search = model_selection.RandomizedSearchCV(clf, param_distributions = param_grid,n_iter = n_iter, cv = cv,\n",
    "                                           iid = False,verbose=1,scoring = scoring,refit = 'Accuracy', n_jobs = -1)\n",
    "        \n",
    "        #print(random_search.get_params)\n",
    "        random_search.fit(X, y)\n",
    "        print(\"best parameters:\", random_search.best_params_)\n",
    "        print(\"%.1f%% accuracy -for AUROC on validation sets (average)\" % (random_search.best_score_*100))\n",
    "        return random_search.best_params_\n",
    "    \n",
    "    def KNN(self,X,y):\n",
    "        print(\"Starting KNN classification- Expected to take about 5 mins as its a hude data set\")\n",
    "       \n",
    "       # X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n",
    "        knn_clf = KNeighborsClassifier()\n",
    "    \n",
    "        param_dist = {'n_neighbors': range(1,100),\"algorithm\" : ['ball_tree', 'kd_tree'],\"weights\" : ['uniform', 'distance'],\"leaf_size\" : range(1,100)}\n",
    "        print(\"Calling random_cv\")\n",
    "        return self.random_CV(knn_clf,X,y,param_dist,4,3)\n",
    "    \n",
    "    def SVM_clf(self,X,y):\n",
    "        print(\"Starting SVM classification\")\n",
    "        #svm_clf = svm.SVC()\n",
    "        svm_clf = svm.SVC()\n",
    "        param_dist = {\n",
    "            'C'     : scipy.stats.reciprocal(1.0, 1000.),\n",
    "            'decision_function_shape' : ['ovo','ovr'],\n",
    "            'kernel': ['linear','rbf','poly'],\n",
    "            'degree' : [2],\n",
    "            'gamma' : scipy.stats.reciprocal(0.01, 10.),\n",
    "            #'break_ties' : [True],\n",
    "            'max_iter' : np.arange(1000,10000,200)\n",
    "            }\n",
    "        return self.random_CV(svm_clf,X,y,param_dist,15,3)\n",
    "    \n",
    "    def DT_clf(self,X,y):\n",
    "        tree_clf = sklearn.tree.DecisionTreeClassifier()\n",
    "        param_dist = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"max_depth\" :[None,500,750,1000,1500,2000],\n",
    "            \"max_features\": [\"sqrt\",\"log2\",None]\n",
    "        }\n",
    "        return self.random_CV(tree_clf,X,y,param_dist,15,5)\n",
    "    \n",
    "    def RF_clf(self,X,y):\n",
    "        print(\"Random Forest Classifier Called\")\n",
    "        rf_clf = sklearn.ensemble.RandomForestClassifier()\n",
    "        param_dist = {\n",
    "            \"n_estimators\" : [10,25,50,75,100,125,150,175,200],\n",
    "            \"max_depth\" :[None,500,750,1000,1500,2000],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"max_features\": [\"sqrt\",\"log2\",None],\n",
    "            \"n_jobs\": [4],\n",
    "           # \"warm_start\" : [True]\n",
    "           # \"bootstrap\": [True,False]\n",
    "        }\n",
    "        return self.random_CV(rf_clf,X,y,param_dist,10,3)\n",
    "    \n",
    "    def ADB_clf(self,X,y):\n",
    "        print(\"AdaBoost Classifier Called\")\n",
    "        adb_clf = sklearn.ensemble.AdaBoostClassifier()\n",
    "        param_dist = {\n",
    "            \"n_estimators\" : [50,100,150,200,300,500,750,1000],\n",
    "            \"algorithm\" : ['SAMME', 'SAMME.R'],\n",
    "           # \"random_state\" : [0]\n",
    "        }\n",
    "        return self.random_CV(adb_clf,X,y,param_dist,5,5)\n",
    "    def LR_clf(self,X,y):\n",
    "        print(\"Logistic Reg Classifier Called\")\n",
    "        lr_clf = sklearn.linear_model.LogisticRegression()\n",
    "        param_dist = {\n",
    "          #  \"fit_intercept\" : [True, False],\n",
    "            'C'     : scipy.stats.reciprocal(1.0, 1000.),\n",
    "            \"solver\" : ['lbfgs','sag','saga','newton-cg'],\n",
    "          #  \"penalty\" : ['l2'],\n",
    "            'multi_class' : ['ovr','multinomial'],\n",
    "            'max_iter' : np.arange(500,10000,500)\n",
    "        }\n",
    "        return self.random_CV(lr_clf,X,y,param_dist,10,3)\n",
    "    \n",
    "    \n",
    "    def NB_clf(self,X,y):\n",
    "        print(\"Naive Bayes Classifier called\")\n",
    "        nb_clf = sklearn.naive_bayes.GaussianNB()\n",
    "        param_dist = {\n",
    "            \"priors\": [None],\n",
    "            'var_smoothing' : scipy.stats.reciprocal(np.exp(-12), np.exp(-5)),\n",
    "        }\n",
    "        return self.random_CV(nb_clf,X,y,param_dist,5,5)\n",
    "    \n",
    "    def MLP_clf(self,X,y):\n",
    "        print(\"Neural Network / MLP Classifier called\")\n",
    "        mlp_clf = sklearn.neural_network.MLPClassifier()\n",
    "        param_dist = {\n",
    "            \"hidden_layer_sizes\" : [(100,), (200,),(100,50),(200,50),(200,100),(100,100,50)],\n",
    "            \"solver\" : ['sgd','lbfgs'],\n",
    "            \"learning_rate\" : ['constant','invscaling'],\n",
    "            \"max_iter\" : [200,300,500],\n",
    "            #\"warm_start\" : [True],\n",
    "            \"activation\" :['tanh', 'relu']\n",
    "        }\n",
    "        return self.random_CV(mlp_clf,X,y,param_dist,6,3)\n",
    "    \n",
    "    def train_clf(self,clf,params,X,y):\n",
    "        clf.set_params(**params)\n",
    "        clf.fit(X,y)\n",
    "        print(\"Complete Training Accuracy\")\n",
    "        print(clf.predict(X[0:5]))\n",
    "        print(clf.score(X,y))\n",
    "        return clf\n",
    "    \n",
    "    def start(self):\n",
    "        \n",
    "        print()\n",
    "        print(\"******Classification of Yeast Data Set Begins ******\")\n",
    "        \n",
    "        file = 'yeast.data'\n",
    "        f = open(file,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        \n",
    "        X_string  = np.char.strip(np.genfromtxt(c,dtype='str',usecols = (9)))\n",
    "        X_float = np.loadtxt(file,usecols = (1,2,3,4,5,6,7,8), dtype = np.float)\n",
    "        \n",
    "        print(X_string[0])\n",
    "        print(X_float[0])\n",
    "        \n",
    "              \n",
    "        #Label binarize of   labels\n",
    "        y,lab_bin = self.preprocess_data_LB(X_string)\n",
    "        \n",
    "        X_train,X_test,y_train,y_test = train_test_split(X_float,y)\n",
    "        \n",
    "        X_raw_train = X_train\n",
    "        X_raw_test = X_test\n",
    "        \n",
    "        X_train,scaler = self.scale_data(X_train)  #using minimax scale as inp data doesnt have much variance\n",
    "        \n",
    "        print(X_train[0])\n",
    "        \n",
    "        \n",
    "       # X = np.column_stack((X_seq,X_string[1:8]))\n",
    "        \n",
    "        print(lab_bin.inverse_transform(y[0:5]))\n",
    "        \n",
    "        print(\"-- Training KNN --\")\n",
    "        knn_clf = self.train_clf(KNeighborsClassifier(),self.KNN(X_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "                                 ,X_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "        \n",
    "        print(\"__Training SVM__\")\n",
    "        svm_clf = self.train_clf(svm.SVC(),self.SVM_clf(X_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "                                 ,X_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "        \n",
    "        print(\"__Training DTree__\")\n",
    "        dt_clf = self.train_clf(sklearn.tree.DecisionTreeClassifier(),self.DT_clf(X_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "                                 ,X_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "        \n",
    "        print(\"__Training RForest__\")\n",
    "        rf_clf = self.train_clf(sklearn.ensemble.RandomForestClassifier(),self.RF_clf(X_raw_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "                              ,X_raw_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "        \n",
    "        print(\"__Training Adaboost__\")\n",
    "        adb_clf = self.train_clf(sklearn.ensemble.AdaBoostClassifier(),self.ADB_clf(X_raw_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "                                 ,X_raw_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "        \n",
    "        print(\"__Training Logistic Reg__\")\n",
    "        lr_clf = self.train_clf(sklearn.linear_model.LogisticRegression(),self.LR_clf(X_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "                                 ,X_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "        \n",
    "        print(\"__Training Naive Bayes__\")\n",
    "        nb_clf = self.train_clf(sklearn.naive_bayes.GaussianNB(),self.NB_clf(X_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "                                 ,X_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "        print(\"__Training MLP__\")\n",
    "        mlp_clf = self.train_clf(sklearn.neural_network.MLPClassifier(),self.MLP_clf(X_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "                                 ,X_train,self.preprocess_data_lbl(lab_bin.inverse_transform(y_train)))\n",
    "        \n",
    "        print(\"**Test Data Prediction Begins*\")\n",
    "        \n",
    "        print(\"Testing KNN Classifier\")\n",
    "        print(knn_clf.score(X_test,self.preprocess_data_lbl(lab_bin.inverse_transform(y_test))))\n",
    "        \n",
    "        print(\"Testing SVM Classifier\")\n",
    "        print(svm_clf.score(X_test,self.preprocess_data_lbl(lab_bin.inverse_transform(y_test))))\n",
    "        \n",
    "        print(\"Testing Decision Trees\")\n",
    "        print(dt_clf.score(X_raw_test,self.preprocess_data_lbl(lab_bin.inverse_transform(y_test)))) #using Raw data\n",
    "        \n",
    "        print(\"Testing Random Forests\")\n",
    "        print(rf_clf.score(X_raw_test,self.preprocess_data_lbl(lab_bin.inverse_transform(y_test))))\n",
    "        \n",
    "        print(\"Testing Adaboost\")\n",
    "        print(adb_clf.score(X_raw_test,self.preprocess_data_lbl(lab_bin.inverse_transform(y_test))))\n",
    "        \n",
    "        print(\"Testing Logistic Regression\")\n",
    "        print(lr_clf.score(X_test,self.preprocess_data_lbl(lab_bin.inverse_transform(y_test))))\n",
    "        \n",
    "        print(\"Testing Naive Bayes\")\n",
    "        print(nb_clf.score(X_test,self.preprocess_data_lbl(lab_bin.inverse_transform(y_test))))\n",
    "        \n",
    "        print(\"Testing Neural Network/MLP\")\n",
    "        print(mlp_clf.score(X_test,self.preprocess_data_lbl(lab_bin.inverse_transform(y_test))))\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class thoracic:\n",
    "    def read(self, a):\n",
    "        f = open(a,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        data, meta = arff.loadarff(c)\n",
    "        return data\n",
    "    \n",
    "    def report(self, results, n_top=3):\n",
    "        print(\"\\n\\n\\n\\n\\n\")\n",
    "        for i in range(1, n_top + 1):\n",
    "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "            j = 0\n",
    "            for candidate in candidates:\n",
    "                print(\"Model with rank: {0}\".format(i))\n",
    "                print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_test_score'][candidate],\n",
    "                      results['std_test_score'][candidate]))\n",
    "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "                print(\"\")\n",
    "                if j > 1:\n",
    "                    break\n",
    "                j+=1\n",
    "\n",
    "\n",
    "    def remove_field(self, data, name):\n",
    "        names = list(data.dtype.names)\n",
    "        if name in names:\n",
    "            names.remove(name)\n",
    "        return data[names]\n",
    "\n",
    "\n",
    "    def randomCV(self, clf, X, y, param_grid, n_iter, cv):\n",
    "        random_search = RandomizedSearchCV(clf, param_distributions = param_grid,\n",
    "                        n_iter = n_iter, cv = cv, iid = False, n_jobs = -1)\n",
    "        random_search.fit(X, y)\n",
    "        self.report(random_search.cv_results_)\n",
    "        return random_search.best_params_\t\n",
    "\n",
    "    def KNN(self, X, y):\n",
    "\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            \"n_neighbors\" : np.arange(1,20),\n",
    "            \"algorithm\" : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            \"weights\" : ['uniform', 'distance'],\n",
    "            \"leaf_size\" : np.arange(1,60)\n",
    "        }\n",
    "        return self.randomCV(neigh, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def SVM(self, X, y):\n",
    "\n",
    "    #        C_grid = [0.1, 1, 10]\n",
    "    #        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "    #        svm_C = svm.SVC(kernel='poly')\n",
    "    #        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid, \"kernel\" : ['poly', 'rbf', 'sigmoid'], }\n",
    "    #        gridcv = GridSearchCV(svm_C, param_grid, verbose=1, cv=3)\n",
    "    #        gridcv.fit(X, y)\n",
    "    #        print(\"best parameters:\", gridcv.best_params_)\n",
    "    #        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "\n",
    "        svm_C = svm.SVC()\n",
    "        param_grid = {\n",
    "            \"kernel\" : ['linear', 'rbf', 'sigmoid'],\n",
    "            \"gamma\" : ['scale', 'auto'],\n",
    "            \"degree\" : np.arange(10),\n",
    "            \"coef0\" : np.random.rand(60)*10,\n",
    "            \"shrinking\" : [False, True],\n",
    "            \"decision_function_shape\" : ['ovo','ovr']\n",
    "        }\n",
    "        return self.randomCV(svm_C, X, y, param_grid, 4, 6)\n",
    "\n",
    "    def DT(self, X, y):\n",
    "        dt = DecisionTreeClassifier()\n",
    "        param_grid = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            \"class_weight\" : [None, 'balanced'],\n",
    "            \"presort\" : [True, False],\n",
    "            \"min_samples_leaf\" : np.arange(1,6)\n",
    "        }\n",
    "        return self.randomCV(dt, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def RF(self, X, y):\n",
    "        rf = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "    #\t\t\"class_weight\" : [None, 'balanced'],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "    #\t\t\"bootstrap\" : [True, False],\n",
    "    #\t\t\"oob_score\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "        }\n",
    "        return self.randomCV(rf, X, y, param_grid, 40, 6)\n",
    "\n",
    "    def Ada(self, X, y):\n",
    "        ada = AdaBoostClassifier(algorithm = \"SAMME\")\n",
    "        param_grid = {\n",
    "    #\t\t\"base_estimator\" : ['classes', 'n_classes_', None],\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)]\n",
    "    #\t\t\"learning_rate\" : [10*x for x in np.random.random_sample((100,))]\n",
    "    #\t\t\"algorithm\" : ['SAMME']\n",
    "        }\n",
    "        return self.randomCV(ada, X, y, param_grid, 40, 6)\n",
    "\n",
    "    def LR(self, X, y):\n",
    "        lr = LogisticRegression()\n",
    "        param_grid = {\n",
    "            \"penalty\" : ['l1', 'l2'],\n",
    "    #\t\t\"dual\" : [True, False],\n",
    "            \"C\" : np.random.rand(60),\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "            \"multi_class\" : ['ovr', 'auto'],\n",
    "            \"solver\" : [ 'liblinear']\n",
    "        }\n",
    "        return self.randomCV(lr, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def GNB(self, X, y):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid = {\n",
    "            \"var_smoothing\" : np.random.random_sample((100,))\n",
    "        }\n",
    "        return self.randomCV(gnb, X, y, param_grid, 100, 6)\n",
    "\n",
    "    def NN(self, X, y):\n",
    "        nn = MLPClassifier()\n",
    "        param_grid = {\n",
    "            \"hidden_layer_sizes\" : np.arange(2,200),\n",
    "            \"activation\" : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            \"solver\" : ['lbfgs', 'sgd', 'adam'],\n",
    "    #\t\t\"verbose\" : [True, False],\n",
    "            \"warm_start\" : [False, True]\n",
    "        }\n",
    "        return self.randomCV(nn, X, y, param_grid, 200, 6)\n",
    "    \n",
    "    def start(self):\n",
    "        data = self.read('9.arff')\n",
    "\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "        y = data[data.dtype.names[-1]]\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Separate target variable\n",
    "        X = self.remove_field(data, data.dtype.names[-1])\n",
    "\n",
    "        le = preprocessing.LabelEncoder()\t\t\t\t\t\t\t\t\t\t\t\t\t# Preprocessing\n",
    "\n",
    "        x = np.empty_like(X[X.dtype.names[1]], dtype = 'float64')\n",
    "\n",
    "        for i in X.dtype.names:\n",
    "            if X[i].dtype != np.float64:\n",
    "                X[i] = le.fit_transform(X[i])\n",
    "                x = np.vstack((x, X[i].astype(np.float64)))\n",
    "            else:\n",
    "                x = np.vstack((x, X[i]))\n",
    "\n",
    "        x = x[1:].T\n",
    "\n",
    "        x_test, x_train = np.split(x, [70])\n",
    "        y_test, y_train = np.split(y, [70])\n",
    "\n",
    "        scaler = StandardScaler()                         # scaling features\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        \n",
    "        \n",
    "        # --------------------> KNN\n",
    "\n",
    "        param = self.KNN(x_train,y_train)\n",
    "        knn_c = KNeighborsClassifier().set_params(**param)\n",
    "        knn_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",knn_c.score(x_test, y_test))\n",
    "\n",
    "        # --------------------> Decision Tree\n",
    "\n",
    "        param = self.DT(x_train,y_train)\n",
    "        dt_c = DecisionTreeClassifier().set_params(**param)\n",
    "        dt_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",dt_c.score(x_test, y_test))\n",
    "        \n",
    "        # --------------------> Random Forest\n",
    "\n",
    "        param = self.RF(x_train,y_train)\n",
    "        rf_c = RandomForestClassifier().set_params(**param)\n",
    "        rf_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",rf_c.score(x_test, y_test))\n",
    "        \n",
    "        # --------------------> Adaboost\n",
    "\n",
    "        param = self.Ada(x_train,y_train)\n",
    "        ada_c = AdaBoostClassifier().set_params(**param)\n",
    "        ada_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",ada_c.score(x_test, y_test))\n",
    "        # ---------------------> Logistic regression\n",
    "\n",
    "        param = self.LR(x_train,y_train)\n",
    "        lr_c = LogisticRegression().set_params(**param)\n",
    "        lr_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",lr_c.score(x_test, y_test))\n",
    "        # ---------------------> Gaussian NB\n",
    "\n",
    "        param = self.GNB(x_train,y_train)\n",
    "        gnb_c = GaussianNB().set_params(**param)\n",
    "        gnb_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",gnb_c.score(x_test, y_test))\n",
    "        \n",
    "        \n",
    "        ############ takes long but do converge #############\n",
    "        \n",
    "#         # ---------------------> Neural Network\n",
    "\n",
    "#         param = self.NN(x_train,y_train)\n",
    "#         nn_c = MLPClassifier().set_params(**param)\n",
    "#         nn_c.fit(x_train, y_train)\n",
    "\n",
    "#         print(\"Score with test data\",nn_c.score(x_test, y_test))\n",
    "#         ## --------------------> SVM\n",
    "\n",
    "#         param = self.SVM(x_train,y_train)\n",
    "#         svm_c = svm.SVC().set_params(**param)\n",
    "#         svm_c.fit(x_train, y_train)\n",
    "\n",
    "#         print(\"Score with test data\",svm_c.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seismic_Bumps:\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def read_data(self):\n",
    "        # reading data and doing 80-20 split\n",
    "        x1 = np.loadtxt('seismic-bumps.csv', delimiter=',', skiprows=1, usecols=(3,4,5,6,8,9,10,11,12,13,14,15,16,17))\n",
    "        x2 = np.loadtxt('seismic-bumps.csv', delimiter=',', skiprows=1, usecols=[0,1,2,7], dtype= '|S1').astype(str)\n",
    "        encoder = OrdinalEncoder()\n",
    "        x2 = encoder.fit_transform(x2)\n",
    "        X = np.column_stack((x1,x2))\n",
    "        y = np.loadtxt('seismic-bumps.csv', delimiter=',', skiprows=1, usecols=(18))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def preprocessing(self, X_train, X_test):\n",
    "        # preprocessing using standard scaler\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        return X_train, X_test\n",
    "    \n",
    "    # Scorer for calculating area under P and R curve\n",
    "    def score_func(y_true, y_pred):\n",
    "        precision, recall, threshold = metrics.precision_recall_curve(y_true, y_pred)\n",
    "        return metrics.auc(recall, precision)\n",
    "        \n",
    "    def cv_SVM(self, X, y):        \n",
    "        scorer = make_scorer(Seismic_Bumps.score_func)        \n",
    "        C_grid = [0.1, 1, 10]\n",
    "        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "        svm = sklearn.svm.SVC(kernel='rbf')\n",
    "        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid, 'kernel' : ['rbf', 'sigmoid']}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(svm, param_grid, n_jobs=-1, verbose=1, cv=3, scoring = scorer, refit=True)\n",
    "        gridcv.fit(X_train, y_train)        \n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% AUPR on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "\n",
    "    def cv_Knn(self, X, y):\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            \"n_neighbors\" : np.arange(1,20),\n",
    "            \"algorithm\" : ['ball_tree', 'kd_tree', 'brute'],\n",
    "            \"weights\" : ['uniform', 'distance'],\n",
    "            \"leaf_size\" : np.arange(1,60)\n",
    "        }\n",
    "        return Seismic_Bumps.randomCV(neigh, X, y, param_grid, 400, 6)\n",
    "    \n",
    "    def cv_DT(self, X, y):\n",
    "        dt = tree.DecisionTreeClassifier()\n",
    "        param_grid = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['sqrt', 'log2', None],\n",
    "            \"class_weight\" : [None, 'balanced'],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "            'max_depth': range(1, 20)\n",
    "        }\n",
    "        return Seismic_Bumps.randomCV(dt, X, y, param_grid, 400, 6)\n",
    "        \n",
    "    def cv_RandomForest(self, X, y):\n",
    "        rf = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['sqrt', 'log2', None],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "            'max_depth': range(1, 20)            \n",
    "        }\n",
    "        return Seismic_Bumps.randomCV(rf, X, y, param_grid, 40, 6)\n",
    "        \n",
    "        \n",
    "    def cv_adaBoost(self, X, y):\n",
    "        scorer = make_scorer(Seismic_Bumps.score_func)\n",
    "        ada_boost = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "        param_grid = {'n_estimators': range(1, 100)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(ada_boost, param_grid, verbose=1, cv=3, scoring=scorer)\n",
    "        gridcv.fit(X, y)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% AUPR on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "        \n",
    "    def cv_logReg(self, X, y):\n",
    "        lr = LogisticRegression()\n",
    "        param_grid = {\n",
    "            \"penalty\" : ['l1', 'l2'],\n",
    "            \"C\" : np.random.rand(60),\n",
    "            \"solver\" : [ 'liblinear']\n",
    "        }\n",
    "        return Seismic_Bumps.randomCV(lr, X, y, param_grid, 400, 6)\n",
    "        \n",
    "    def cv_GNB(self, X, y):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid = {\n",
    "            \"var_smoothing\" : np.random.random_sample((100,))\n",
    "        }\n",
    "        return Seismic_Bumps.randomCV(gnb, X, y, param_grid, 100, 6)\n",
    "    \n",
    "    def cv_NN(self, X, y):\n",
    "        nn = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                                           solver='sgd', batch_size=100, max_iter=10,\n",
    "                                           learning_rate_init=.01, momentum=0.9, alpha=0.05,\n",
    "                                           verbose=False, random_state=0)\n",
    "\n",
    "        param_grid ={\n",
    "                    'hidden_layer_sizes' : range(2,100),\n",
    "                    \"activation\" : ['identity', 'logistic', 'tanh', 'relu']\n",
    "                    }\n",
    "        return Seismic_Bumps.randomCV(nn, X, y, param_grid, 200, 6)\n",
    "        \n",
    "    def randomCV(clf, X, y, param_grid, n_iter, cv):\n",
    "        scorer = make_scorer(Seismic_Bumps.score_func)\n",
    "        random_search = RandomizedSearchCV(clf, param_distributions = param_grid, n_iter = n_iter, cv = cv, iid = False, \n",
    "                                           scoring = scorer)\n",
    "        random_search.fit(X, y)\n",
    "        print(random_search.best_params_)        \n",
    "        Seismic_Bumps.report(random_search.cv_results_)\n",
    "        return random_search.best_estimator_\n",
    "    \n",
    "    def report(results, n_top=1):\n",
    "        for i in range(1, n_top + 1):\n",
    "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "            k = 0\n",
    "            for candidate in candidates:                                \n",
    "                print(\"Model with rank: {0}\".format(i))\n",
    "                print(\"AUPR on validation data: {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_test_score'][candidate],\n",
    "                      results['std_test_score'][candidate]))\n",
    "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "                print(\"\")\n",
    "                k += 1\n",
    "                if k == 3:\n",
    "                    break\n",
    "    \n",
    "    def predict(self, model, X_test, y_test):\n",
    "        predict = model.predict(X_test)\n",
    "        accuracy = sklearn.metrics.accuracy_score(y_test, predict)\n",
    "        print(\"Final Accuracy on test data : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------SVM--------\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'C': 10, 'gamma': 0.01}\n",
      "72.7% accuracy on validation sets (average)\n",
      "Final Accuracy on test data :  0.7012987012987013\n",
      "---------KNN--------\n",
      "Fitting 3 folds for each of 19 candidates, totalling 57 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'n_neighbors': 19}\n",
      "64.5% accuracy on validation sets (average)\n",
      "Final Accuracy on test data :  0.6320346320346321\n",
      "---------Decision Tree--------\n",
      "Fitting 3 folds for each of 19 candidates, totalling 57 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'max_depth': 3}\n",
      "63.4% accuracy on validation sets (average)\n",
      "Final Accuracy on test data :  0.6233766233766234\n",
      "---------Random Forest--------\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:   12.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'n_jobs': 1}\n",
      "65.1% accuracy on validation sets (average)\n",
      "Final Accuracy on test data :  0.6363636363636364\n",
      "---------AdaBoost--------\n",
      "Fitting 3 folds for each of 99 candidates, totalling 297 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 297 out of 297 | elapsed:   45.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'n_estimators': 65}\n",
      "68.5% accuracy on validation sets (average)\n",
      "Final Accuracy on test data :  0.6623376623376623\n",
      "---------Logistic Regression--------\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'C': 183.29807108324337}\n",
      "75.9% accuracy on validation sets (average)\n",
      "Final Accuracy on test data :  0.7316017316017316\n",
      "---------Gaussian Naive Bayes--------\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "best parameters: {}\n",
      "63.2% accuracy on validation sets (average)\n",
      "Final Accuracy on test data :  0.6060606060606061\n",
      "---------Neural Network--------\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Iteration 1, loss = 0.77977984\n",
      "Iteration 2, loss = 0.74672835\n",
      "Iteration 3, loss = 0.71764804\n",
      "Iteration 4, loss = 0.70102722\n",
      "Iteration 5, loss = 0.68854207\n",
      "Iteration 6, loss = 0.67911807\n",
      "Iteration 7, loss = 0.67120614\n",
      "Iteration 8, loss = 0.66555845\n",
      "Iteration 9, loss = 0.65975534\n",
      "Iteration 10, loss = 0.65407613\n",
      "Iteration 1, loss = 0.79067072\n",
      "Iteration 2, loss = 0.74210285\n",
      "Iteration 3, loss = 0.70643218\n",
      "Iteration 4, loss = 0.68633316\n",
      "Iteration 5, loss = 0.67141092\n",
      "Iteration 6, loss = 0.66031359\n",
      "Iteration 7, loss = 0.65247897\n",
      "Iteration 8, loss = 0.64626903\n",
      "Iteration 9, loss = 0.64085351\n",
      "Iteration 10, loss = 0.63607822\n",
      "Iteration 1, loss = 0.77521956\n",
      "Iteration 2, loss = 0.73963943\n",
      "Iteration 3, loss = 0.70774791\n",
      "Iteration 4, loss = 0.68585298\n",
      "Iteration 5, loss = 0.67319715\n",
      "Iteration 6, loss = 0.66129354\n",
      "Iteration 7, loss = 0.65336647\n",
      "Iteration 8, loss = 0.64684116\n",
      "Iteration 9, loss = 0.64184770\n",
      "Iteration 10, loss = 0.63773493\n",
      "Iteration 1, loss = 0.83153200\n",
      "Iteration 2, loss = 0.73620464\n",
      "Iteration 3, loss = 0.66063278\n",
      "Iteration 4, loss = 0.64387579\n",
      "Iteration 5, loss = 0.63646629\n",
      "Iteration 6, loss = 0.62934969\n",
      "Iteration 7, loss = 0.62287874\n",
      "Iteration 8, loss = 0.61701745\n",
      "Iteration 9, loss = 0.61306138\n",
      "Iteration 10, loss = 0.60898696\n",
      "Iteration 1, loss = 0.85269799\n",
      "Iteration 2, loss = 0.70592047\n",
      "Iteration 3, loss = 0.63388896\n",
      "Iteration 4, loss = 0.61837304\n",
      "Iteration 5, loss = 0.61404747\n",
      "Iteration 6, loss = 0.60989006\n",
      "Iteration 7, loss = 0.60474132\n",
      "Iteration 8, loss = 0.59862621\n",
      "Iteration 9, loss = 0.59370600\n",
      "Iteration 10, loss = 0.59085491\n",
      "Iteration 1, loss = 0.84107422\n",
      "Iteration 2, loss = 0.72753630\n",
      "Iteration 3, loss = 0.64761492\n",
      "Iteration 4, loss = 0.62846016\n",
      "Iteration 5, loss = 0.62133212\n",
      "Iteration 6, loss = 0.61663225\n",
      "Iteration 7, loss = 0.61114145\n",
      "Iteration 8, loss = 0.60554152\n",
      "Iteration 9, loss = 0.60106068\n",
      "Iteration 10, loss = 0.59762623\n",
      "Iteration 1, loss = 0.74000810\n",
      "Iteration 2, loss = 0.68735607\n",
      "Iteration 3, loss = 0.66728180\n",
      "Iteration 4, loss = 0.65738904\n",
      "Iteration 5, loss = 0.64604136\n",
      "Iteration 6, loss = 0.63702569\n",
      "Iteration 7, loss = 0.62984425\n",
      "Iteration 8, loss = 0.62544572\n",
      "Iteration 9, loss = 0.61988956\n",
      "Iteration 10, loss = 0.61538039\n",
      "Iteration 1, loss = 0.73857422\n",
      "Iteration 2, loss = 0.66968374\n",
      "Iteration 3, loss = 0.64436779\n",
      "Iteration 4, loss = 0.63639497\n",
      "Iteration 5, loss = 0.62797970\n",
      "Iteration 6, loss = 0.61972769\n",
      "Iteration 7, loss = 0.61698923\n",
      "Iteration 8, loss = 0.61192945\n",
      "Iteration 9, loss = 0.60580170\n",
      "Iteration 10, loss = 0.60158900\n",
      "Iteration 1, loss = 0.75628962\n",
      "Iteration 2, loss = 0.68903386\n",
      "Iteration 3, loss = 0.65362293\n",
      "Iteration 4, loss = 0.64439243\n",
      "Iteration 5, loss = 0.63846773\n",
      "Iteration 6, loss = 0.63188532\n",
      "Iteration 7, loss = 0.62599792\n",
      "Iteration 8, loss = 0.62178107\n",
      "Iteration 9, loss = 0.61761751\n",
      "Iteration 10, loss = 0.61307353\n",
      "Iteration 1, loss = 0.73658616\n",
      "Iteration 2, loss = 0.70331224\n",
      "Iteration 3, loss = 0.68282938\n",
      "Iteration 4, loss = 0.67366711\n",
      "Iteration 5, loss = 0.66286349\n",
      "Iteration 6, loss = 0.65313577\n",
      "Iteration 7, loss = 0.64592430\n",
      "Iteration 8, loss = 0.63986694\n",
      "Iteration 9, loss = 0.63371806\n",
      "Iteration 10, loss = 0.62864989\n",
      "Iteration 1, loss = 0.71431404\n",
      "Iteration 2, loss = 0.67336545\n",
      "Iteration 3, loss = 0.66198668\n",
      "Iteration 4, loss = 0.65303107\n",
      "Iteration 5, loss = 0.64317280\n",
      "Iteration 6, loss = 0.63349357\n",
      "Iteration 7, loss = 0.62613199\n",
      "Iteration 8, loss = 0.62101049\n",
      "Iteration 9, loss = 0.61674138\n",
      "Iteration 10, loss = 0.61285225\n",
      "Iteration 1, loss = 0.73743894\n",
      "Iteration 2, loss = 0.69604644\n",
      "Iteration 3, loss = 0.67211193\n",
      "Iteration 4, loss = 0.66443414\n",
      "Iteration 5, loss = 0.65341145\n",
      "Iteration 6, loss = 0.64396052\n",
      "Iteration 7, loss = 0.63598104\n",
      "Iteration 8, loss = 0.63096583\n",
      "Iteration 9, loss = 0.62533036\n",
      "Iteration 10, loss = 0.62086240\n",
      "Iteration 1, loss = 0.70360105\n",
      "Iteration 2, loss = 0.66885193\n",
      "Iteration 3, loss = 0.65212068\n",
      "Iteration 4, loss = 0.64362923\n",
      "Iteration 5, loss = 0.63625660\n",
      "Iteration 6, loss = 0.62989751\n",
      "Iteration 7, loss = 0.62250837\n",
      "Iteration 8, loss = 0.61787149\n",
      "Iteration 9, loss = 0.61253232\n",
      "Iteration 10, loss = 0.60933650\n",
      "Iteration 1, loss = 0.69997047\n",
      "Iteration 2, loss = 0.64857602\n",
      "Iteration 3, loss = 0.63258818\n",
      "Iteration 4, loss = 0.62348487\n",
      "Iteration 5, loss = 0.61646270\n",
      "Iteration 6, loss = 0.60986481\n",
      "Iteration 7, loss = 0.60327248\n",
      "Iteration 8, loss = 0.59848291\n",
      "Iteration 9, loss = 0.59423068\n",
      "Iteration 10, loss = 0.59053754\n",
      "Iteration 1, loss = 0.71068251\n",
      "Iteration 2, loss = 0.66439375\n",
      "Iteration 3, loss = 0.64488931\n",
      "Iteration 4, loss = 0.63542570\n",
      "Iteration 5, loss = 0.62850432\n",
      "Iteration 6, loss = 0.62194903\n",
      "Iteration 7, loss = 0.61565546\n",
      "Iteration 8, loss = 0.61058050\n",
      "Iteration 9, loss = 0.60593926\n",
      "Iteration 10, loss = 0.60268785\n",
      "Iteration 1, loss = 0.81513195\n",
      "Iteration 2, loss = 0.66954746\n",
      "Iteration 3, loss = 0.63081207\n",
      "Iteration 4, loss = 0.62142349\n",
      "Iteration 5, loss = 0.61418084\n",
      "Iteration 6, loss = 0.60821678\n",
      "Iteration 7, loss = 0.60230399\n",
      "Iteration 8, loss = 0.59818945\n",
      "Iteration 9, loss = 0.59337539\n",
      "Iteration 10, loss = 0.58900576\n",
      "best parameters: {'hidden_layer_sizes': 30}\n",
      "66.3% accuracy on validation sets (average)\n",
      "Final Accuracy on test data :  0.658008658008658\n",
      "******Classification of Credit card Default Data Set Begins ******\n",
      "READING TRAIN DATA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23364,)\n",
      "(30000,)\n",
      "-- Training KNN --\n",
      "Starting KNN classification training- Expected to take about 3 mins as its a hude data set\n",
      "Calling random_cv\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score + Accuracy + AUPR\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'weights': 'uniform', 'n_neighbors': 22, 'leaf_size': 53, 'algorithm': 'ball_tree'}\n",
      "63.9% accuracy -for AUROC on validation sets (average)\n",
      " \n",
      "**Complete Training Accuracy Score**\n",
      "0.8215\n",
      "__Training SVM__\n",
      "Starting SVM classification\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score + Accuracy + AUPR\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'C': 206.05664973447384, 'degree': 2, 'gamma': 0.04515119523364441, 'kernel': 'rbf', 'max_iter': 13800}\n",
      "65.0% accuracy -for AUROC on validation sets (average)\n",
      " \n",
      "**Complete Training Accuracy Score**\n",
      "0.7695\n",
      "__Training DTree__\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score + Accuracy + AUPR\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'splitter': 'best', 'max_features': None, 'max_depth': 1500, 'criterion': 'entropy'}\n",
      "62.0% accuracy -for AUROC on validation sets (average)\n",
      " \n",
      "**Complete Training Accuracy Score**\n",
      "0.9992916666666667\n",
      "__Training RForest__\n",
      "Random Forest Classifier Called\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score + Accuracy + AUPR\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'warm_start': True, 'n_jobs': 4, 'n_estimators': 175, 'max_features': None, 'max_depth': 2000, 'criterion': 'entropy'}\n",
      "66.3% accuracy -for AUROC on validation sets (average)\n",
      " \n",
      "**Complete Training Accuracy Score**\n",
      "0.9992916666666667\n",
      "__Training Adaboost__\n",
      "AdaBoost Classifier Called\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score + Accuracy + AUPR\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'n_estimators': 750, 'algorithm': 'SAMME.R'}\n",
      "64.4% accuracy -for AUROC on validation sets (average)\n",
      " \n",
      "**Complete Training Accuracy Score**\n",
      "0.8226666666666667\n",
      "__Training Logistic Reg__\n",
      "Logistic Reg Classifier Called\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score + Accuracy + AUPR\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'C': 3.153861858037794, 'fit_intercept': True, 'max_iter': 8000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "59.9% accuracy -for AUROC on validation sets (average)\n",
      " \n",
      "**Complete Training Accuracy Score**\n",
      "0.8078333333333333\n",
      "__Training Naive Bayes__\n",
      "Naive Bayes Classifier called\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score + Accuracy + AUPR\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  25 | elapsed:    0.6s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'priors': [0.4, 0.6]}\n",
      "70.2% accuracy -for AUROC on validation sets (average)\n",
      " \n",
      "**Complete Training Accuracy Score**\n",
      "0.7506666666666667\n",
      "__Training MLP__\n",
      "Neural Network / MLP Classifier called\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score + Accuracy + AUPR\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "        #Calling Retinopathy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = Retinopathy()\n",
    "    X_train, y_train, X_test, y_test = obj.read_data()\n",
    "    X_train, X_test = obj.preprocessing(X_train, X_test)\n",
    "    print('---------SVM--------')\n",
    "    model = obj.cv_SVM(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------KNN--------')\n",
    "    model = obj.cv_Knn(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Decision Tree--------')\n",
    "    model = obj.cv_DT(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Random Forest--------')\n",
    "    model = obj.cv_RandomForest(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------AdaBoost--------')\n",
    "    model = obj.cv_adaBoost(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Logistic Regression--------')\n",
    "    model = obj.cv_logReg(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Gaussian Naive Bayes--------')\n",
    "    model = obj.cv_GNB(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Neural Network--------')\n",
    "    model = obj.cv_NN(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    \n",
    "\n",
    "    #Calling Credit Card setup\n",
    "cr_obj = credit_card_defaults()\n",
    "cr_obj.start()\n",
    "\n",
    "\n",
    "\n",
    "#Calling Breast cancer setup\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = Breast_Cancer()\n",
    "    X_train, y_train, X_test, y_test = obj.read_data()\n",
    "    X_train, X_test = obj.preprocessing(X_train, X_test)\n",
    "    print('---------SVM--------')\n",
    "    model = obj.cv_SVM(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------KNN--------')\n",
    "    model = obj.cv_Knn(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Decision Tree--------')\n",
    "    model = obj.cv_DT(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Random Forest--------')\n",
    "    model = obj.cv_RandomForest(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------AdaBoost--------')\n",
    "    model = obj.cv_adaBoost(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Logistic Regression--------')\n",
    "    model = obj.cv_logReg(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Gaussian Naive Bayes--------')\n",
    "    model = obj.cv_GNB(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Neural Network--------')\n",
    "    model = obj.cv_NN(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    \n",
    "    #calling Statlog(Aus)\n",
    "    \n",
    "    \n",
    "stat = statlog_aus()\n",
    "stat.start()\n",
    "\n",
    "# Calling Statlog (German)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = German_CC()\n",
    "    X_train, y_train, X_test, y_test = obj.read_data()\n",
    "    print('---------SVM--------')\n",
    "    model = obj.cv_SVM(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------KNN--------')\n",
    "    model = obj.cv_Knn(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Decision Tree--------')\n",
    "    model = obj.cv_DT(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Random Forest--------')\n",
    "    model = obj.cv_RandomForest(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------AdaBoost--------')\n",
    "    model = obj.cv_adaBoost(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Logistic Regression--------')\n",
    "    model = obj.cv_logReg(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Gaussian Naive Bayes--------')\n",
    "    model = obj.cv_GNB(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Neural Network--------')\n",
    "    model = obj.cv_NN(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "\n",
    "#calling steel\n",
    "\n",
    "stee = steel()\n",
    "stee.start()\n",
    "\n",
    "#Calling Adult\n",
    "adult = Adult()\n",
    "adult.start()\n",
    "\n",
    "#Calling YEast\n",
    "\n",
    "yeast_obj = yeast()\n",
    "yeast_obj.start()\n",
    "\n",
    "#calling Thoraic\n",
    "\n",
    "\n",
    "thor = thoracic()\n",
    "thor.start()\n",
    "\n",
    "\n",
    "#calling Seismic Bumps\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = Seismic_Bumps()\n",
    "    X_train, y_train, X_test, y_test = obj.read_data()\n",
    "    X_train, X_test = obj.preprocessing(X_train, X_test)\n",
    "    print('---------SVM--------')\n",
    "    model = obj.cv_SVM(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------KNN--------')\n",
    "    model = obj.cv_Knn(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Decision Tree--------')\n",
    "    model = obj.cv_DT(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Random Forest--------')\n",
    "    model = obj.cv_RandomForest(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------AdaBoost--------')\n",
    "    model = obj.cv_adaBoost(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Logistic Regression--------')\n",
    "    model = obj.cv_logReg(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Gaussian Naive Bayes--------')\n",
    "    model = obj.cv_GNB(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Neural Network--------')\n",
    "    model = obj.cv_NN(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
